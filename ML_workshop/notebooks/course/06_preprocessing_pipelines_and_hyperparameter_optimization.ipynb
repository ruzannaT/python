{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Kameron');\n",
       "    @import url('http://fonts.googleapis.com/css?family=Crimson+Text');\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Lato');\n",
       "    @import url('http://fonts.googleapis.com/css?family=Source+Sans+Pro');\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Lora'); \n",
       "\n",
       "    \n",
       "    body {\n",
       "        font-family: 'Lora', Consolas, sans-serif;\n",
       "       \n",
       "        -webkit-print-color-adjust: exact important !;\n",
       "        \n",
       "      \n",
       "       \n",
       "    }\n",
       "    \n",
       "    .alert-block {\n",
       "        width: 95%;\n",
       "        margin: auto;\n",
       "    }\n",
       "    \n",
       "    .rendered_html code\n",
       "    {\n",
       "        color: black;\n",
       "        background: #eaf0ff;\n",
       "        background: #f5f5f5; \n",
       "        padding: 1pt;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    \n",
       "    p {\n",
       "      line-height: 140%;\n",
       "    }\n",
       "    \n",
       "    strong code {\n",
       "        background: red;\n",
       "    }\n",
       "    \n",
       "    .rendered_html strong code\n",
       "    {\n",
       "        background: #f5f5f5;\n",
       "    }\n",
       "    \n",
       "    .CodeMirror pre {\n",
       "    font-family: 'Source Code Pro', monocco, Consolas, monocco, monospace;\n",
       "    }\n",
       "    \n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        font-weight: normal;\n",
       "     }\n",
       "     \n",
       "     strong {\n",
       "         background: #f5f5f5;\n",
       "         margin-top: 4pt;\n",
       "         margin-bottom: 4pt;\n",
       "         padding: 2pt;\n",
       "         border: 0.5px solid #a0a0a0;\n",
       "         font-weight: bold;\n",
       "         color: darkred;\n",
       "     }\n",
       "     \n",
       "    \n",
       "    div #notebook {\n",
       "        # font-size: 10pt; \n",
       "        line-height: 145%;\n",
       "        }\n",
       "        \n",
       "    li {\n",
       "        line-height: 145%;\n",
       "    }\n",
       "\n",
       "    div.output_area pre {\n",
       "        background: #fff9d8 !important;\n",
       "        padding: 5pt;\n",
       "       \n",
       "       -webkit-print-color-adjust: exact; \n",
       "        \n",
       "    }\n",
       " \n",
       "    \n",
       " \n",
       "    h1, h2, h3, h4 {\n",
       "        font-family: Kameron, arial;\n",
       "\n",
       "\n",
       "    }\n",
       "    \n",
       "    div#maintoolbar {display: none !important;}\n",
       "</style>\n",
       "    <script>\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "        return false;\n",
       "}\n",
       "    </script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "from IPython.core.display import HTML; HTML(open(\"custom.html\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Preprocessing, pipelines and hyperparameters optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About transformations / preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen before that adding polynomial features to the 2D `xor` and `circle` problem made both tasks treatable by a simple linear classifier.\n",
    "\n",
    "Comment: we use *transformation* and *preprocessing* interchangably.\n",
    "\n",
    "Beyond adding polynomial features, there are other important preprocessors / transformers to mention:\n",
    "\n",
    "\n",
    "### Scaler\n",
    "\n",
    "A scaler applies a linear transformation on every feature. Those transformations are individual per column.\n",
    "\n",
    "The two most important ones in `scikit-learn` are\n",
    "\n",
    "- `MinMaxScaler`:  after applying this scaler, the minumum in every column is 0, the maximum is 1.\n",
    "\n",
    "- `StandardScaler`: scales columns to mean value 0 and standard deviation 1.\n",
    "\n",
    "The reason to use a scaler is to compensate for different orders of magnitudes of the features. Some classifiers like `SVC` and `KNeighborsClassifier` use eucledian distances between features internally which would impose more weight on features having large values. So **don't forget to scale your features when using SVC or KNeighborsClassifier** !\n",
    "\n",
    "\n",
    "### PCA\n",
    "\n",
    "Principal component analysis is a technique to reduce the dimensionality of a multi variate data set. One benefit of PCA is to remove redundancy in your data set, such as correlating columns or linear dependencies between columns.\n",
    "\n",
    "We discussed before that reducing redundancy and noise can help to avoid overfitting.\n",
    "\n",
    "\n",
    "### Function transformers\n",
    "\n",
    "It can help to apply functions like `log` or `exp` or `1/x` to features to improve classification performance.\n",
    "\n",
    "Lets assume you want to forecast the outcome of car crash experiments and one variable is the time $t$ needed for the distance $l$ from start to crash. Transforming this to the actual speed $\\frac{l}{t}$ could be a more informative feature then $t$.\n",
    "\n",
    "### Imputing missing values\n",
    "\n",
    "Sometimes data contain missing values. Data imputation is a strategy to fill up missing values, e.g. by the columnwise mean or by applying another strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we demonstrante how a scaler can be implemented. Our scaling strategy will scale given values to the range 0 to 1.\n",
    "\n",
    "First we create a random data matrix and compute columnwise min and max values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values: [-2.50919762  9.01428613  4.63987884  1.97316968 -6.87962719]\n",
      "\n",
      "min value: -6.87962719115127\n",
      "max value: 9.014286128198322\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# for repducible numbers:\n",
    "np.random.seed(42)\n",
    "\n",
    "values = np.random.random((5,)) * 20 - 10\n",
    "\n",
    "min_value = values.min()\n",
    "max_value = values.max()\n",
    "\n",
    "print(\"values:\", values)\n",
    "print()\n",
    "print(\"min value:\", min_value)\n",
    "print(\"max value:\", max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy for scaling is as follows: Our values $v$ are in the range $v_{min}$ to $v_{max}$:\n",
    "\n",
    "$$\n",
    "v_{min} \\le  v  \\le v_{max}\n",
    "$$\n",
    "\n",
    "\n",
    "Then subtracting $v_{min}$ results in \n",
    "\n",
    "$$\n",
    "0 \\le  v - v_{min} \\le v_{max} - v_{min}\n",
    "$$\n",
    "  \n",
    "Finally dividing by the right hand side delivers the property we are looking for:\n",
    "\n",
    "$$\n",
    "0 \\le \\frac{v - v_{min}}{v_{max} - v_{min}} \\le 1\n",
    "$$\n",
    "\n",
    "\n",
    "In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled values: [0.27497505 1.         0.72477469 0.5569929  0.        ]\n"
     ]
    }
   ],
   "source": [
    "scaled_values = (values - min_value) / (max_value - min_value)\n",
    "\n",
    "print(\"scaled values:\", scaled_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that all values are now scaled as intended.\n",
    "\n",
    "To apply the same strategy column per column to a feature matrix, `scikit-learn` offers a `MinMaxScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.88010959 -8.83832776  7.32352292]\n",
      " [ 2.02230023  4.16145156 -9.58831011]\n",
      " [ 9.39819704  6.64885282 -5.75321779]\n",
      " [-6.36350066 -6.3319098  -3.91515514]\n",
      " [ 0.49512863 -1.36109963 -4.1754172 ]]\n"
     ]
    }
   ],
   "source": [
    "features = np.random.random((5, 3)) * 20 - 10\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         1.        ]\n",
      " [0.54688796 0.83938966 0.        ]\n",
      " [1.         1.         0.22676976]\n",
      " [0.03173604 0.16183823 0.33545476]\n",
      " [0.45307159 0.48280112 0.32006542]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# learning -> determine columnwise min/max values\n",
    "scaler = MinMaxScaler().fit(features)\n",
    "\n",
    "# transformation ! -> apply linear transformation based on min/max values:\n",
    "print(scaler.transform(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         1.        ]\n",
      " [0.54688796 0.83938966 0.        ]\n",
      " [1.         1.         0.22676976]\n",
      " [0.03173604 0.16183823 0.33545476]\n",
      " [0.45307159 0.48280112 0.32006542]]\n"
     ]
    }
   ],
   "source": [
    "# shorter !\n",
    "print(scaler.fit_transform(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can divide preprocessing into two classes:\n",
    "\n",
    "1. Preprocessing which depends on the full data set. E.g.\n",
    "\n",
    "   - Scaling\n",
    "   - PCA\n",
    "   - Many variants for imputation of missing values\n",
    "   \n",
    "\n",
    "2. Preprocessing which can be applied row per row individually. E.g.\n",
    "\n",
    "   - Adding polynomial features\n",
    "   - Functional transforms\n",
    "   - Row-wise scaling (e.g. when a row represents an image and we want to compensate for different illumination).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<h3><i class=\"fa fa-info-circle\"></i>&nbsp;Important</h3>\n",
    "\n",
    " When we include preprocessing in a classification approach, we must later apply **exactly the same preprocessing** on new incoming data!\n",
    "\n",
    "For preprocessors which depend on the full data set this implies that we never must preprocess data before cross-validation !\n",
    "\n",
    "Running such preprocessors on the full data set lets information of \"unseen\" data sneak into the classifier.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is how we must proceed instead:\n",
    "\n",
    "In case for the `MinMaxScaler`:\n",
    "\n",
    "1. Determine columnwise minimum und maximum values of training features.\n",
    "2. Use these to scale training features.\n",
    "3. Learn Classifier.\n",
    "\n",
    "\n",
    "4. Use values from 1. to scale evaluation features (thus we might create values outside `0..1`).\n",
    "5. Apply classifier to evaluation features.\n",
    "6. Assess Performance.\n",
    "\n",
    "In general:\n",
    "\n",
    "1. Learn prprocessor `P` on training data set.\n",
    "2. Apply `P` on training data set.\n",
    "3. Learn classifier `C` on the training data set.\n",
    "\n",
    "\n",
    "4. Apply `P` from before to the evaluation data set.\n",
    "5. Apply classifier `C` on the scaled evaluation data set.\n",
    "6. Assess performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/2xi5wt.jpg\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The scikit-learn API (quick recap)\n",
    "\n",
    "We've seen before that we can swap `scikit-learn` classifiers easily without changing much code. \n",
    "\n",
    "This is possible, because all classifiers have methods `.fit` and `.predict` which also have the same function signature (this means number and meaning of arguments is always the same for every implementation of `.fit` respectively `.predict`.)\n",
    "\n",
    "This consistend design within `scikit-learn` also applies for preprocessors transformers, which all have methods`.fit`, `.transform` and `.fit_transform`.\n",
    "\n",
    "This consistent API allows setting up **processing pipelines**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "A so called classifiation pipeline consists of 0 or more pre processors plus a final classifier.\n",
    "\n",
    "Let us start with the following pipeline:\n",
    "\n",
    "1. Use PCA to reduce data to 3 dimensions\n",
    "2. Apply scaling to mean 0 and std deviation 1\n",
    "3. Train `SVC` classifier.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "p = make_pipeline(PCA(3), StandardScaler(), SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a pipeline now \"behaves\" like a single classifier, as it implements `.fit` and `.predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p.fit     True\n",
      "p.predict True\n"
     ]
    }
   ],
   "source": [
    "print(\"p.fit    \", p.fit is not None)\n",
    "print(\"p.predict\", p.predict is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this we can also use cross-validation in the same way as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9330127360562145\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "beer_data = pd.read_csv(\"beers.csv\")\n",
    "\n",
    "features = beer_data.iloc[:, :-1]\n",
    "labels = beer_data.iloc[:, -1]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(p, features, labels, scoring=\"accuracy\", cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\"></i>&nbsp;One benefit of using a pipeline is that you will  not mistakenly scale the full data set first, instead we follow the strategy we described above automatically.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to setup a good pipeline ?\n",
    "\n",
    "Regrettably there is no recipe how to setup a good performing classification pipeline except reasonable preprocessing, especially feature engineering. After that it is up to experimentation and the advice on how to choose classifiers we gave in the last script.\n",
    "\n",
    "Let us try out different pipeplines and evaluate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863 ['svc']\n",
      "0.947 ['standardscaler', 'svc']\n",
      "0.915 ['minmaxscaler', 'svc']\n",
      "0.804 ['logisticregression']\n",
      "0.924 ['standardscaler', 'pca', 'logisticregression']\n",
      "0.893 ['standardscaler', 'pca', 'logisticregression']\n",
      "0.840 ['polynomialfeatures', 'svc']\n",
      "0.925 ['polynomialfeatures', 'logisticregression']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for p in [make_pipeline(SVC()),\n",
    "          make_pipeline(StandardScaler(), SVC()),\n",
    "          make_pipeline(MinMaxScaler(), SVC()),\n",
    "          make_pipeline(LogisticRegression()),\n",
    "          make_pipeline(StandardScaler(), PCA(3), LogisticRegression()),\n",
    "          make_pipeline(StandardScaler(), PCA(2), LogisticRegression()),\n",
    "\n",
    "\n",
    "          make_pipeline(PolynomialFeatures(), SVC()),\n",
    "          make_pipeline(PolynomialFeatures(), LogisticRegression()),\n",
    "          \n",
    "         ]:\n",
    "    \n",
    "    print(\"{:.3f}\".format(cross_val_score(p, features, labels, scoring=\"accuracy\", cv=5).mean()), end=\" \")\n",
    "    print([pi[0] for pi in p.steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise session:\n",
    "\n",
    "1. Can you come up with a better performing classification pipeline ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (*) Optional exercise:\n",
    "\n",
    "Build a classification pipeline to classifiy the 2D xor- and circle-data sets with linear classifiers. Also assess their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\"></i>&nbsp;Up to now we applied preprocessing to the full feature table. `scikit-learn` also allows preprocessing of single columns or a subset of them. the concept in `scikit-learn` is called `ColumnTransformer`, more about this\n",
    "[can be found here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "Classifiers and pipelines have parameters which must be adapted for improving performance (e.g. `gamma` or `C`). Finding good parameters is also called *hyperparameter optimization* to distinguish from the optimization done during learning of many classification algorithms.\n",
    "\n",
    "### Up to now we adapted such hyperparameters manually, but there are more systematic approaches !\n",
    "\n",
    "<img src=\"https://i.imgflip.com/3040hg.jpg\" title=\"made at imgflip.com\" width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach is to specify valid values for each parameter involved and then try out all possible combinations. This is called *grid search*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9822222222222222 {'C': 5, 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# optimize parameters of one single classifier\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly'), \n",
    "              'C':[1, 5, 10, 15]\n",
    "              }\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "# run gridsearch, use CV to assess quality and determine best parameter \n",
    "# set:\n",
    "\n",
    "# tries all 3 x 4 = 12 combinations:\n",
    "search = GridSearchCV(svc, parameters, cv=5)\n",
    "search.fit(features, labels)\n",
    "\n",
    "print(search.best_score_, search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such an optimization can also be applied to a full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = make_pipeline(PolynomialFeatures(), StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specification of the grid id now a bit more complicated: \n",
    "\n",
    "- first the name of the processor / classifier in lower case letters\n",
    "- then two underscores `__` \n",
    "- finally the name of the argument of the processor / classifier.\n",
    "\n",
    "`StandardScaler` e.g. has parameters `with_mean` and `with_std` which can be `True` or `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3, 4],\n",
    "              'standardscaler__with_mean': [True, False],\n",
    "              'standardscaler__with_std': [True, False],\n",
    "              'logisticregression__C': [.1, .5, 1, 10, 20],\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid has `4 x 2 x 2 x 5` thus `80` points. So we muss run crossvalidation for 80 different classifiers.\n",
    "\n",
    "To speed this up, we can specify `n_jobs = 2` to use `2` extra processor cores to run gridsearch in parallel (you might want to use more cores depending on your computer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.982):\n",
      "{'logisticregression__C': 10, 'polynomialfeatures__degree': 3, 'standardscaler__with_mean': True, 'standardscaler__with_std': True}\n"
     ]
    }
   ],
   "source": [
    "search = GridSearchCV(p, param_grid, cv=4, scoring=\"accuracy\", n_jobs=2)\n",
    "search.fit(features, labels)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more efficient, approach is `RandomizedSearchCV`. \n",
    "\n",
    "In this case we can also specify random distributions for the parameters to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_dist = {'polynomialfeatures__degree': randint(1, 4),\n",
    "              'standardscaler__with_mean': [True, False],\n",
    "              'standardscaler__with_std': [True, False],\n",
    "              'logisticregression__C': uniform(.1, 20)\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run now 30 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.982):\n",
      "{'logisticregression__C': 17.31461166512687, 'polynomialfeatures__degree': 3, 'standardscaler__with_mean': False, 'standardscaler__with_std': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "search = RandomizedSearchCV(p, param_dist, cv=4, n_jobs=2, n_iter=30)\n",
    "\n",
    "search.fit(features, labels)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise section 2\n",
    "\n",
    "1. Try to find good parameters for the following two pipelines applied to the beer data set. Use grid search as well as randomized search for both.\n",
    "\n",
    "    `make_pipeline(StandardScaler(), SVC(gamma=..., C=...))`\n",
    "    \n",
    "    `make_pipeline(StandardScaler(), PolynomialFeatures(degree=..), PCA(n_components=...), LinearSVC())`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
