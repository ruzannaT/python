{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Kameron');\n",
       "    @import url('http://fonts.googleapis.com/css?family=Crimson+Text');\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Lato');\n",
       "    @import url('http://fonts.googleapis.com/css?family=Source+Sans+Pro');\n",
       "    \n",
       "    @import url('http://fonts.googleapis.com/css?family=Lora'); \n",
       "\n",
       "    \n",
       "    body {\n",
       "        font-family: 'Lora', Consolas, sans-serif;\n",
       "       \n",
       "        -webkit-print-color-adjust: exact important !;\n",
       "        \n",
       "      \n",
       "       \n",
       "    }\n",
       "    \n",
       "    .alert-block {\n",
       "        width: 95%;\n",
       "        margin: auto;\n",
       "    }\n",
       "    \n",
       "    .rendered_html code\n",
       "    {\n",
       "        color: black;\n",
       "        background: #eaf0ff;\n",
       "        background: #f5f5f5; \n",
       "        padding: 1pt;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    \n",
       "    p {\n",
       "      line-height: 140%;\n",
       "    }\n",
       "    \n",
       "    strong code {\n",
       "        background: red;\n",
       "    }\n",
       "    \n",
       "    .rendered_html strong code\n",
       "    {\n",
       "        background: #f5f5f5;\n",
       "    }\n",
       "    \n",
       "    .CodeMirror pre {\n",
       "    font-family: 'Source Code Pro', monocco, Consolas, monocco, monospace;\n",
       "    }\n",
       "    \n",
       "    .cm-s-ipython span.cm-keyword {\n",
       "        font-weight: normal;\n",
       "     }\n",
       "     \n",
       "     strong {\n",
       "         background: #f5f5f5;\n",
       "         margin-top: 4pt;\n",
       "         margin-bottom: 4pt;\n",
       "         padding: 2pt;\n",
       "         border: 0.5px solid #a0a0a0;\n",
       "         font-weight: bold;\n",
       "         color: darkred;\n",
       "     }\n",
       "     \n",
       "    \n",
       "    div #notebook {\n",
       "        # font-size: 10pt; \n",
       "        line-height: 145%;\n",
       "        }\n",
       "        \n",
       "    li {\n",
       "        line-height: 145%;\n",
       "    }\n",
       "\n",
       "    div.output_area pre {\n",
       "        background: #fff9d8 !important;\n",
       "        padding: 5pt;\n",
       "       \n",
       "       -webkit-print-color-adjust: exact; \n",
       "        \n",
       "    }\n",
       " \n",
       "    \n",
       " \n",
       "    h1, h2, h3, h4 {\n",
       "        font-family: Kameron, arial;\n",
       "\n",
       "\n",
       "    }\n",
       "    \n",
       "    div#maintoolbar {display: none !important;}\n",
       "</style>\n",
       "    <script>\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "        return false;\n",
       "}\n",
       "    </script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IGNORE THIS CELL WHICH CUSTOMIZES LAYOUT AND STYLING OF THE NOTEBOOK !\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings = lambda *a, **kw: None\n",
    "from IPython.core.display import HTML; HTML(open(\"custom.html\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Metrics for evaluating the performance of a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we used _accuracy_, the percentage of correct classifcations, to evaluate the quality of a classifier.\n",
    "\n",
    "Regrettably _accuracy_ can produce very misleading results. \n",
    "\n",
    "This and the next chapter will discuss other metrics  to asses the quality of a classifier including the possible pitfalls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define the **confusion matrix** we must introduce some additional terms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying a classifier to a data set with known labels `0` and `1`:\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<h3><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</h3>\n",
    "<ul>\n",
    "\n",
    "<li><strong>TP (true positives)</strong>: labels which were predicted as <code>1</code> and actually are <code>1</code>. <br/><br/>\n",
    "\n",
    "\n",
    "<li><strong>TN (true negatives)</strong>: labels which were predicted as <code>0</code> and actually are <code>0</code>.<br/><br/>\n",
    "\n",
    "\n",
    "<li><strong>FP (false positives)</strong>: labels which were predicted as <code>1</code> and actually are <code>0</code>.<br/><br/>\n",
    "\n",
    "\n",
    "<li><strong>FN (false negatives)</strong>: labels which were predicted as <code>0</code> and actually are <code>1</code>.<br/><br/>\n",
    "\n",
    "</ul>\n",
    "\n",
    "To memorize this: \n",
    "\n",
    "<ul>\n",
    "\n",
    "<li>The second word \"positives\"/\"negatives\" refers to the prediction computed by the classifier.\n",
    "<li>The first word \"true\"/\"false\" expresses if the classification was correct or not.\n",
    "\n",
    "</ul>\n",
    "\n",
    "This is the so called <strong>Confusion Matrix</strong>:\n",
    "\n",
    "<table style=\"border: 1px; font-family: 'Source Code Pro', monocco, Consolas, monocco, monospace;\n",
    "              font-size:110%;\">\n",
    "    <tbody >\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\"> </td>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual P</td>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual N</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted P</td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TP         </td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FP         </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted N</td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FN         </td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TN         </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "</div>\n",
    "\n",
    "<img src=\"https://i.imgflip.com/305c8j.jpg\" title=\"made at imgflip.com\" width=40%/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- So the total number of predictions can be expressed as `TP` + `FP` + `FN` + `TN`.\n",
    "\n",
    "\n",
    "- The number of correct predictions is `TP` + `TN`.\n",
    "\n",
    "\n",
    "- `TP` + `FN` is the number of positive examples in our data set, \n",
    "\n",
    "\n",
    "- `FP` + `TN` is the number of negative examples.\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</h3>\n",
    "\n",
    "This allows us to define <strong>accuracy</strong> as (<code>TP</code> + <code>TN</code>) / (<code>TP</code> + <code>FP</code> + <code>FN</code> + <code>TN</code>).\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pitfalls\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-info-circle\"></i>&nbsp; Accuracy can be very misleading if classe sizes are imbalanced\n",
    "</div>\n",
    "\n",
    "\n",
    "Let us demonstrate this with an extreme example:\n",
    "\n",
    "- On average 10 out of 10000 people are infected with a disease `X`. \n",
    "- A medical test `Z` diagnoses 50 % of infected people as `not infected` ?\n",
    "- The test is correct on all  not-infected people.\n",
    "\n",
    "\n",
    "Among $10000$ people \n",
    "\n",
    "- $10$ will be infected, $5$ gets a correct result.\n",
    "- $9990$ will be not infected with a correct test result.\n",
    "\n",
    "Thus accuracy is $\\frac{9995}{10000} = 99.95 \\% $\n",
    "\n",
    "\n",
    "This is also called the **accuracy paradox** (<a href=\"https://en.wikipedia.org/wiki/Accuracy_paradox\">see also here</a>).\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgflip.com/303wyp.jpg\" title=\"made at imgflip.com\" width=50%/>\n",
    "\n",
    "\n",
    "\n",
    "To evaluate this test on such an unbalanced dataset we need different numbers: \n",
    "\n",
    "1. Does our test miss infected people: How many infected people are actually discovered to be infected ?\n",
    "\n",
    "2. Does our test predict people as infected which are actually not: How many positive diagnoses are correct ?\n",
    "\n",
    "We come back to this example later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise block 1\n",
    "\n",
    "1.1 A classifier predicts labels `[0, 0, 1, 1, 1, 0, 1, 1]` whereas true labels are `[0, 1, 0, 1, 1, 0, 1, 0]`. First write these values as a two columned table using pen & paper and assign `FP`, `TP`, ... to each row. Now create the confusion matrix and compute accuracy.\n",
    "\n",
    "1.2 A random classfier just assign a randomly chosen label `0` or `1` for a given feature. What is the average accuracy of such a classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise\n",
    "\n",
    "1.3 Assume the previously described test also produces wrong results on not-infected people, such that 5 out of 10000 will be diagnosed as infected. Compute the confusion matrix and the accuracy of this test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Precision and Recall\n",
    "\n",
    "In order to understand the concept of **precision** and **recall**, imagine the following scenario:\n",
    "\n",
    "A few days before thanksgiving you open an online recipe website and enter \"turkey thanksgiving\". You see some suitable recommendations but also unusable results related to Turkish recipes.\n",
    "\n",
    "Such a search engine works like a filter applied on a collection of documents.\n",
    "\n",
    "As a scientist you want to assess the reliablity of this service:\n",
    "\n",
    "1. What fraction of relevant recipes stored in the underlying database do I see?\n",
    "\n",
    "2. How many of the shown results are relevant recipes and not the recipes from Turkey?\n",
    "\n",
    "\n",
    "\n",
    "In this context, **recall** is the fraction of all the relevant documents found by the engine.\n",
    "\n",
    "And **precision** is the fraction of shown results that are correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute precision and recall for a classifier\n",
    "\n",
    "To transfer this concept to classification, we can interpret a classifier as a filter. The classifier classifies every  document in a collection as relevant or not relevant.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<h3><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</h3>\n",
    "\n",
    "To remember:\n",
    "\n",
    "<table style=\"border: 1px; font-family: 'Source Code Pro', monocco, Consolas, monocco, monospace;\n",
    "              font-size:110%;\">\n",
    "    <tbody >\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\"> </td>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual P</td>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Actual N</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted P</td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TP         </td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FP         </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 10px; background:#f8f8f8;\">Predicted N</td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">FN         </td>\n",
    "            <td style=\"padding: 10px; background:#fcfcfc; text-align:center; font-weight: bold\">TN         </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "The number of shown documents is  <code>TP + FP </code>, the number of relevant documents is <code>TP + FN</code>\n",
    "\n",
    "Thus: \n",
    "\n",
    "- **precision** is computed as <code>TP / (TP + FP)</code>.\n",
    "\n",
    "\n",
    "- **recall** is computed as <code>TP / (TP + FN)</code>.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "The confusion matrix for the medical test `Z` is then:\n",
    "\n",
    "\n",
    "<table style=\"border: 1px solid black\">\n",
    "    <tr style=\"border: 1px black\">\n",
    "        <td style=\"border: 1px  solid black; background: white; padding: 1em\">TP = 5</td>\n",
    "        <td style=\"border: 1px  solid black; background: white; \">FP = 0</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px black\">\n",
    "        <td style=\"border: 1px solid black; background: white; padding: 1em \">FN = 5</td>\n",
    "        <td style=\"border: 1px solid black; background: white; \">TN = 9900</td>\n",
    "    </tr>\n",
    "        \n",
    "</table>\n",
    "\n",
    "Here precision is `1.0` and recall is `0.5`.\n",
    "\n",
    "\n",
    "### Trade-off between precision and recall.\n",
    "\n",
    "The more results the search engine delivers, the lesser will be the number of relevant documents which are ignored. But at the same time the fraction of wrong results will increase.\n",
    "\n",
    "\n",
    "### F1-score\n",
    "\n",
    "Sometimes we want a single number instead of two numbers to compare the performace of multiple classifiers.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><i class=\"fa fa-info-circle\"></i>&nbsp;Definition</h3>\n",
    "    \n",
    "The **F1 score** is computed as\n",
    "<code>F1 = 2 * (precision * recall) / (precision + recall)</code>.\n",
    "\n",
    "This is the *harmonic mean* of precision and recall.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "For the medical test `Z` the `F1` score is `1 / 1.5 = 0.6666..`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise block 2\n",
    "\n",
    "Use your results from exercise 1.1 to compute precision, recall and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise:\n",
    "\n",
    "Compute precision, recall and F1-score for the test described in exercise 1.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other metrics\n",
    "\n",
    "The discussion above was just a quick introduction to measuring the accuracy of a classifier. We skipped other metrics such as `ROC` and `AUC` amongst others.\n",
    "\n",
    "A good introduction to `ROC` <a href=\"https://classeval.wordpress.com/introduction/introduction-to-the-roc-receiver-operating-characteristics-plot/\">can be found here.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metrics in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.metrics` contains many metrics like `precision_score` etc., `confusion_matrix` prints what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [1 4]]\n",
      "\n",
      "precision            0.800\n",
      "recall               0.800\n",
      "f1                   0.800\n",
      "accuracy             0.750\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (precision_score, recall_score, f1_score, \n",
    "                             confusion_matrix, accuracy_score)\n",
    "\n",
    "# these numbers are from exercise 1.1:\n",
    "predicted = [0, 0, 1, 1, 1, 0, 1, 1]\n",
    "labels =  [0, 1, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "print(confusion_matrix(labels, predicted))\n",
    "print()\n",
    "\n",
    "#\n",
    "# The first argument of the metrics functions is the exact labels, \n",
    "# the second argument is the predictions:\n",
    "#\n",
    "\n",
    "print(\"{:20s} {:.3f}\".format(\"precision\", precision_score(labels, predicted)))\n",
    "print(\"{:20s} {:.3f}\".format(\"recall\", recall_score(labels, predicted)))\n",
    "print(\"{:20s} {:.3f}\".format(\"f1\", f1_score(labels, predicted)))\n",
    "print(\"{:20s} {:.3f}\".format(\"accuracy\", accuracy_score(labels, predicted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `cross_val_score` (introduced in the last script) allows to use other metrics than `accuracy`.\n",
    "\n",
    "We demonstrate usage of different metrics on two data sets:\n",
    "\n",
    "- the known beer data samples in which labels distribution is almost 50:50.\n",
    "- an unbalanced subset of the beer data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "beer_data = pd.read_csv(\"beers.csv\")\n",
    "print(beer_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced data\n",
      "52.9 % of the beers are yummy\n",
      "\n",
      "   accuracy    : mean value: 0.80\n",
      "   f1          : mean value: 0.83\n",
      "   precision   : mean value: 0.78\n",
      "   recall      : mean value: 0.89\n",
      "\n",
      "unbalanced data\n",
      "26.9 % of the beers are yummy\n",
      "\n",
      "   accuracy    : mean value: 0.79\n",
      "   f1          : mean value: 0.41\n",
      "   precision   : mean value: 0.87\n",
      "   recall      : mean value: 0.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def assess(classifier, beer_data):\n",
    "    features = beer_data.iloc[:, :-1]\n",
    "    labels = beer_data.iloc[:, -1]\n",
    "    n = len(labels)\n",
    "    print(\"{:.1f} % of the beers are yummy\".format(100 * sum(labels == 1) /n))\n",
    "    print()\n",
    "    \n",
    "    for metric in [\"accuracy\", \"f1\", \"precision\", \"recall\"]:\n",
    "        scores = cross_val_score(classifier, features, labels, scoring=metric, cv=5)\n",
    "        print(\"   {:12s}: mean value: {:.2f}\".format(metric, scores.mean()))\n",
    "    \n",
    "    print()\n",
    "\n",
    "    \n",
    "classifier = LogisticRegression(C=1)  \n",
    "\n",
    "print(\"balanced data\")\n",
    "assess(classifier, beer_data)\n",
    "\n",
    "# we sort by label, then removing samples| is easier:\n",
    "beer_data = beer_data.sort_values(by=\"is_yummy\")\n",
    "\n",
    "print(\"unbalanced data\")\n",
    "beer_data_unbalanced = beer_data.iloc[:-80, :]\n",
    "assess(classifier, beer_data_unbalanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that for the balanced data set the values for `f1` and for `accuracy` are almost equal, but differ significantly for the unbalanced data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise section 3\n",
    "\n",
    "1. Play with the previous examples, use different classifiers with different settings\n",
    "\n",
    "### Optional exercise\n",
    "\n",
    "2. Modify the code from section 5 of the previous script (\"Training the final classifier\") to use different metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
